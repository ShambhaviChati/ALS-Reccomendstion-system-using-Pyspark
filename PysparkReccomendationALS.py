# -*- coding: utf-8 -*-
"""IDS561Lab4_2020spr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJcBtXpcSX_8zv_OZeFYr4Hu3h__A5a6

# Spark installation
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null

!wget -q http://apache.mirrors.hoobly.com/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz

!ls

!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.5-bin-hadoop2.7"

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("Python Spark SQL basic example") \
    .config("spark.some.config.option", "some-value") \
    .getOrCreate()

# importing ALS from pyspark.ml which works on Dataframes
from pyspark.ml.recommendation import ALS 
# Libraries we'll use for evaluation
from pyspark.sql.types import FloatType
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import col
from pyspark.ml.recommendation import ALS
from pyspark.sql.functions import expr
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.sql.functions import lit
from pyspark import SparkContext
sc = SparkContext.getOrCreate()
from pyspark.sql.window import Window
from pyspark.sql.functions import rank, col

df = sc.textFile('u.data')
df = df.map(lambda x: x.split("\t"))
columns = ["user", "item","rating","timestamp"]
# convert to spark data frame
df = spark.createDataFrame(df).toDF(*columns)
type(df)
for col_name in columns:
    df = df.withColumn(col_name, col(col_name).cast('int'))
# splitting into train and test sets
X_train, X_test = df.randomSplit([0.6, 0.4])

# Creating and training a basic model
als = ALS(rank=5, maxIter=10, seed=0, userCol= 'user', itemCol='item', ratingCol="rating",coldStartStrategy="drop",nonnegative=True)
model = als.fit(X_train.select(["user", "item", "rating"]))

#Evaluating predictions
evaluator=RegressionEvaluator(metricName="rmse",labelCol="rating",predictionCol="prediction")
predictions=model.transform(X_test)
rmse=evaluator.evaluate(predictions)
print("RMSE="+str(rmse))
predictions.show()
# RMSE of 0.945

#ALS 4
def repeatALS(data, k=4, userCol= 'user', itemCol='item', ratingCol='rating', metricName='rmse',predictionCol='prediction'):
    evaluations = []
    for i in range(0, k):  
        (trainingSet, testingSet) = data.randomSplit([k - 1.0, 1.0])
        als = ALS(userCol=userCol, itemCol=itemCol, ratingCol=ratingCol)
        model = als.fit(trainingSet)
        predictions = model.transform(testingSet)
        evaluator = RegressionEvaluator(metricName=metricName, labelCol='rating', predictionCol='prediction')
        evaluation = evaluator.evaluate(predictions.na.drop())
        print('Loop {}: {} = {}'.format(i + 1, metricName, evaluation))
        evaluations.append(evaluation)
    return sum(evaluations) / float(len(evaluations))

print('RMSE = {}'.format(repeatALS(df, k=4)))

#4-fold Cross Validation
def kfoldALS(data, k=4, userCol= 'user', itemCol='item', ratingCol='rating', metricName='rmse'):
    evaluations = []
    weights = [1.0] * k
    splits = data.randomSplit(weights)
    for i in range(0, k):  
        testingSet = splits[i]
        trainingSet = spark.createDataFrame(sc.emptyRDD(), data.schema)
        for j in range(0, k):
            if i == j:
                continue
            else:
                trainingSet = trainingSet.union(splits[j])
        als = ALS(userCol=userCol, itemCol=itemCol, ratingCol=ratingCol)
        model = als.fit(trainingSet)
        predictions = model.transform(testingSet)
        evaluator = RegressionEvaluator(metricName=metricName, labelCol='rating', predictionCol='prediction')
        evaluation = evaluator.evaluate(predictions.na.drop())
        print('Loop {}: {} = {}'.format(i + 1, metricName, evaluation))
        evaluations.append(evaluation)
    return sum(evaluations) / float(len(evaluations))

print('RMSE = {}'.format(kfoldALS(df, k=4)))

(trainingRatings, validationRatings) = df.randomSplit([80.0, 20.0])
evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')

paramGrid = ParamGridBuilder().addGrid(als.rank, [1, 5, 10]).addGrid(als.maxIter, [20]).addGrid(als.regParam, [0.05, 0.1, 0.5]).build()

crossval = CrossValidator(estimator=als, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=10)
cvModel = crossval.fit(trainingRatings)
predictions = cvModel.transform(validationRatings)

print('The root mean squared error for our model is: {}'.format(evaluator.evaluate(predictions.na.drop())))

predictions.show()

type(predictions)

#Final Reccomendation Function
def recommendMovies(model, user, nbRecommendations):
    # Create a Spark DataFrame with the specified user and all the movies listed in the ratings DataFrame
    dataSet = df.select('item').distinct().withColumn('user', lit(user))

    # Create a Spark DataFrame with the items that have already been rated by this user
    itemrated = df.filter(df.user == user).select('item', 'user')

    # Apply the recommender system to the data set without the already rated items to predict ratings
    predictions = model.transform(dataSet.subtract(itemrated)).dropna().orderBy('prediction', ascending=False).limit(nbRecommendations).select('item', 'prediction')

    # Join with the  DataFrame to get the movies titles and genres
    recommendations = predictions.join(df, predictions.item == df.item).select( predictions.item)

#     recommendations.show(truncate=False)
    return recommendations

print('Recommendations for user 133:')
recommendMovies(model, 133, 10).show(10)

#creating window to partition and get top 10 predictions per user
window = Window.partitionBy(predictions['user']).orderBy(predictions['prediction'].desc())
#top 10 predicted ratings for each user
top_10 = predictions.select('*', rank().over(window).alias('rank')).filter(col('rank') <= 10)

#Top_10 predicted reccomendtion for each user
print(top_10)
top_10.repartition(1).write.format("com.databricks.spark.csv").option("header", "true").save("output.csv")